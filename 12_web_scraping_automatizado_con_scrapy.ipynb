{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagenes/pythonista.png](imagenes/pythonista.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El proyecto *Scrapy*.\n",
    "\n",
    "Existen diversa herramientas y técnicas que le permiten a un desarrollador o analista acceder, consumir y extraer contenidos basado en Web. El proyecto *Scrapy* ofrece una herramienta que permite realizar 'web scraping' de manera automatizada y rápida de grand cantidades de contenido basado en web.\n",
    "\n",
    "*Scrapy* cuenta con muy buena documentación, la cual puede ser accedida desde https://doc.scrapy.org/en/latest.\n",
    "\n",
    "*Scrapy* fue creado a partir de [*Tiwsted*](https://twistedmatrix.com), por lo que es capaz de realizar miles de consultas de forma simultánea.\n",
    "\n",
    "Del mismo modo, *Scrapy* hace uso de herramientas como *beautifulsoup* y el paquete *xml* de Python para facilitar la búsqueda de contenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo yum install libxml2-devel python-lxml -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez instalado, es posible utilizar el comando *scrapy* desde la línea de comando, utuilizando a su vez subcomandos.\n",
    "En caso de no ingresar un subcomando, *scrapy* desplegará una lista básica de subcomandos\n",
    "disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sitio de ejemplo.\n",
    "\n",
    "El uso de herramientas automatizadas para la adquisición de contenido web (web crawling) es una práctica que en algunos casos no es aceptable para ciertos sitios en internet. Con la finalidad de evitar inconvenientes, hemos puesto a disposición el dominio http://coder.mx, el cual presenta un sitio ficticio que puede ser utilizado para practicar el uso de herramientas como *scrapy* sin inconvenientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de un proyecto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder comenzar, es necesario crear un proyecto desde la línea de comando utilizando el subcomando *startproject*:\n",
    "\n",
    "``` bash\n",
    "scrapy startproject <nombre>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!scrapy startproject prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo anterior creará una estructura de directorios y archivos en el sitio indicado dentro del sistema de archivos local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El comando *tree* nos permite observar la estructura de archivos que fue creada. Para instalar *tree*n en un sistema Linux basado en Red Hat se ejecuta los siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! sudo yum install tree -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de 'arañas'.\n",
    "A las herramientas encargadas de escudriñar los sitios web se les conoce como 'arañas' y para crear una se utiliza el subcomando *genspider* desde la línea de comando.\n",
    "\n",
    "```\n",
    "scrapy genspider <nombre de la araña> <URL>\n",
    "```\n",
    "\n",
    "Un proyecto de Scrapy puede hacer uso de una o múltiples arañas.\n",
    "\n",
    "#### Ejemplo:\n",
    "Se creará una araña que accederá al sitio http://coder.mx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy genspider coder coder.mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez creada, la araña será alojada en el subdirectorio *spiders* del proyecto y por defecto contendrá un código similar al siguiente:\n",
    "``` python\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class PythonistaSpider(scrapy.Spider):\n",
    "    name = 'pythonista'\n",
    "    allowed_domains = ['coder.mx']\n",
    "    start_urls = ['http://coder.mx/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cat prueba/spiders/coder.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El paquete *scrapy*.\n",
    "Como se puede ver en el código previo, el paquete *scrapy* contiene todas las herramientas de *Scrapy*, el cual define diversas clases y herramientas que facilitan realizar web scraping dse forma automatizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(scrapy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La clase *scrapy.Spider*.\n",
    "\n",
    "Todas las arañas de *scrapy* son subclases de la clase *scrapy.Spider*, la cual define por defecto los siguientes atributos y métodos:\n",
    "\n",
    "* El atributo *name*, el cual es el nombre que se el asignará a cada objeto instanciado.\n",
    "* *allowed_domains*, que se refiere a un objeto tipo _list_ que contiene la lista de los dominios a los que puede acceder la araña.\n",
    "* El atributo *start_urls*, que corresponde a una lista de URLs a partir de los cuales las arañas escudriñarán el contenido.\n",
    "* El método *parse()*, el cual en un principio es un método abstracto y se utiliza para definir los contenidos que debe de buscar la araña.\n",
    "* El método *start_requests()* se encarga de iniciar la carga de datos a partir de los URL ingresados en *start_urls* y obteniendo los datos definidos en *parse*. El resultado es un objeto llamado *Request*.\n",
    "\n",
    "Existen otros atributos y métodos definidos para *scrapy* que se verán posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(scrapy.Spider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicio del proceso de adquisición de contenidos.\n",
    "En este caso, se realizará una búsqueda en http://coder.mx, utilizando la araña localizada en [prueba/prueba/spiders/coder.py](prueba/prueba/spiders/coder.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy crawl coder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al inciar a \"arrastrarse\" en el sitio indicado, *scrapy* busca por defecto un archivo *robots.txt* en el sitio que está escudriñando. Estos archivos son utilizados por los administradores de los sitios web para definir a qué partes del sitio tiene acceso las arañas.\n",
    "En este caso, del http://coder.mx/robots.txt no existe, por lo que el servidor regresa un mensaje de estado *404*.\n",
    "En vista de que se está ejecutando una búsqueda recién creada, no hay reglas definidas, por lo que *scrapy* sólo realiza escudriña al contenido correspondiente a http://coder.mx/index.html, pero no realiza búsquedas en ningún otro archivo o subdirectorio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución de una araña.\n",
    "\n",
    "En este caso se ejecutará la araña localizada en [prueba/prueba/spiders/coder.py](prueba/prueba/spiders/coder.py), la cual al no contenr criterios refinados de búsqueda, obtendrá y regresará el contenido completo de http://coder.mx/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy fetch --spider=coder https://coder.mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selectores.\n",
    "\n",
    "Los selectores son los objetos que se encargan de realizar búsquedas específicas dentro de un texto estructurado y son instancias del objetos *scrapy.Selector*.\n",
    "\n",
    "La clase *scrapy.Selector* contiene varios métodos, entre los que se incluyen:\n",
    "* *css()*, permite realizar búsqueda de nodos dentro de un documento HTML mediante la sintaxis de selectores de CSS.\n",
    "* *xpath()*, permite realizar búsquedas de nodos dentro de un documento HTML/XML mediante XPath.\n",
    "* *re()*,  permite realizar búsquedas de nodos dentro de un documento HTML/XML mediante expresiones regulares.\n",
    "* *extract()* regresa los datos correspondientes a un nodo encontrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(scrapy.Selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## El shell de *scrapy*.\n",
    "\n",
    "Scrapy cuenta con su propio entorno interactivo, el cual es muy similar al shell de Python. Este entorno nos permite probar las búsquedas que se pueden realizar en un sitio específico.\n",
    "\n",
    "Este shell permite al usuario interactuar con los objetos y elementos de un proyecto.\n",
    "\n",
    "Lo único que se requiere es ejecutar lo siguiente desde la línea de comando:\n",
    "\n",
    "``` bash\n",
    "scrapy shell <URL>```\n",
    "\n",
    "**Ejemplo:**\n",
    "\n",
    "**Advertencia:** este ejemplo se realiza desde una terminal de texto. No intente ejecutar los comandos desde esta notebook.\n",
    "\n",
    "En la página [http://coder.mx/menu.html](http://coder.mx/menu.html) existe una estructura de tablas que contienen una serie de elementos *dt* que corresponden al nombre de cada platillo en el menú de un restaruante ficticio y del mismo modo existen un elemento *td* con el atributo *class=\"precio\"*, que corresonden al precio de cada producto.\n",
    "\n",
    "Para acceder al shell y conectarse a la página sobre la que se haran las búsquedas se ejectua lo siguiente desde una terminal localizada en el subdirectorio del proyecto de scrapy:\n",
    "\n",
    "``` bash\n",
    "scrapy shell http://coder.mx/menu.html\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo anterior dará por resultado algo similar a lo siguiente y se iniciará el shell de *scrapy*:\n",
    "\n",
    "```\n",
    "18-03-07 14:30:28 [scrapy.utils.log] INFO: Scrapy 1.5.0 started (bot: prueba)\n",
    "2018-03-07 14:30:28 [scrapy.utils.log] INFO: Versions: lxml 4.1.1.0, libxml2 2.9.7, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twi\n",
    "sted 17.9.0, Python 3.6.4 (default, Dec 19 2017, 14:48:12) - [GCC 4.8.5 20150623 (Red Hat 4.8.5-16)], pyOpenSSL 17.5.0 (OpenSSL 1.1.\n",
    "0g  2 Nov 2017), cryptography 2.1.4, Platform Linux-3.10.0-693.17.1.el7.x86_64-x86_64-with-centos-7.4.1708-Core\n",
    "2018-03-07 14:30:28 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'prueba', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseD\n",
    "upeFilter', 'LOGSTATS_INTERVAL': 0, 'NEWSPIDER_MODULE': 'prueba.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['prueba.spiders\n",
    "']}\n",
    "2018-03-07 14:30:28 [scrapy.middleware] INFO: Enabled extensions:\n",
    "['scrapy.extensions.corestats.CoreStats',\n",
    " 'scrapy.extensions.telnet.TelnetConsole',\n",
    " 'scrapy.extensions.memusage.MemoryUsage']\n",
    "2018-03-07 14:30:28 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
    "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
    " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
    " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
    " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
    " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
    " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
    " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
    "2018-03-07 14:30:28 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
    "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
    " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
    " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
    " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
    " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
    "2018-03-07 14:30:28 [scrapy.middleware] INFO: Enabled item pipelines:\n",
    "[]\n",
    "2018-03-07 14:30:28 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
    "2018-03-07 14:30:28 [scrapy.core.engine] INFO: Spider opened\n",
    "2018-03-07 14:30:28 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://coder.mx/robots.txt> (referer: None)\n",
    "2018-03-07 14:30:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://coder.mx/menu.html> (referer: None)\n",
    "[s] Available Scrapy objects:\n",
    "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
    "[s]   crawler    <scrapy.crawler.Crawler object at 0x7f624cd9e550>\n",
    "[s]   item       {}\n",
    "[s]   request    <GET http://coder.mx/menu.html>\n",
    "[s]   response   <200 http://coder.mx/menu.html>\n",
    "[s]   settings   <scrapy.settings.Settings object at 0x7f624b3a6e48>\n",
    "[s]   spider     <CoderSpider 'coder' at 0x7f624aed6fd0>\n",
    "[s] Useful shortcuts:\n",
    "[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n",
    "[s]   fetch(req)                  Fetch a scrapy.Request and update local objects\n",
    "[s]   shelp()           Shell help (print this help)\n",
    "[s]   view(response)    View response in a browser\n",
    "In [1]:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar el comando, scrapy se conectó a la página y generó una petición con el método *GET*. El objeto correspondiente a la respuesta es una instancia de la clase *HtmlResponse* y fue ligado al nombre *response*.\n",
    "A su vez, las clase *HtmlResponse* es subclase de *scrapy.http.response.text.TextResponse*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(scrapy.http.response.text.TextResponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dicha clase contiene los métodos *css()* y *xpath()*, los cuales nos permiten hacer búsquedas en el el código html de la respuesta.\n",
    "\n",
    "El siguiente código utilizando el método *xpath()* traerá la lista de elementos HTML que son *dt*.\n",
    "``` python\n",
    "response.xpath('//dt')\n",
    "```\n",
    "El resultado es un objeto tipo *SelectorList* que contiene los resultados de la búsqueda: \n",
    "```\n",
    "In [1]: response.xpath('//dt')\n",
    "Out[1]:\n",
    "[<Selector xpath='//dt' data='<dt><em>Ensalada mixta</em></dt>'>,\n",
    " <Selector xpath='//dt' data='<dt><em>Corazones de alcachofa al vapor<'>,\n",
    " <Selector xpath='//dt' data='<dt><em>Empanadas argentinas (3)</em></d'>,\n",
    " <Selector xpath='//dt' data='<dt><em>Sashimi de pepino</em></dt>'>,\n",
    " <Selector xpath='//dt' data='<dt><em>Consomé de la Condesa</em></dt>'>,\n",
    " <Selector xpath='//dt' data='<dt><em>Jugo de carne</em></dt>'>,\n",
    " <Selector xpath='//dt' data='<dt><em>Sopa de lentejas</em></dt>'>,\n",
    " <Selector xpath='//dt' data='<dt><em>Sopa de fideos</em></dt>'>,\n",
    " <Selector xpath='//dt' data='<dt><em>Pechuga en salsa de huitlacoche<'>,\n",
    " <Selector xpath='//dt' data='<dt><em>Nuestras gringas muy mexicanas</'>,\n",
    " <Selector xpath='//dt' data='<dt><em>Pepitos de arrachera</em></dt>'>,\n",
    " <Selector xpath='//dt' data='<dt><em>Huachinango a la sal</em></dt>'>,\n",
    " <Selector xpath='//dt' data='<dt><em>Pan de pulque</em></dt>'>,\n",
    " <Selector xpath='//dt' data='<dt><em>Nieve de leche quemada con tuna<'>,\n",
    " <Selector xpath='//dt' data='<dt><em>Sorbete de rosas y vino blanco e'>,\n",
    " <Selector xpath='//dt' data='<dt><em>Crepas al momento</em></dt>'>]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para desplegar el texto de cada nodo encontrado se aplica el siguiente código:\n",
    "\n",
    "``` python\n",
    "for item in response.xpath('//dt'):\n",
    "    print(item.extract())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código utilizando el método *css()* traerá la lista de elementos HTML que son de clase *precio*.\n",
    "\n",
    "El resultado es un objeto de tipo _list_ que contiene otro objeto tipo _list_ cuyos elementos son un selector y el elemento HTML que coincide con la búsqueda.\n",
    "```\n",
    " [2]: response.css('.precio')\n",
    "Out[2]:\n",
    "[<Selector xpath=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' precio ')]\" data='<td class=\"precio\">$65.00</td>'>,\n",
    " <Selector xpath=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' precio ')]\" data='<td class=\"precio\">$125.00</td>'>,\n",
    " <Selector xpath=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' precio ')]\" data='<td class=\"precio\">$130.00</td>'>,\n",
    " <Selector xpath=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' precio ')]\" data='<td class=\"precio\">$150.00</td>'>,\n",
    " <Selector xpath=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' precio ')]\" data='<td class=\"precio\">$85.00</td>'>,\n",
    " <Selector xpath=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' precio ')]\" data='<td class=\"precio\">$85.00</td>'>,\n",
    " <Selector xpath=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' precio ')]\" data='<td class=\"precio\">$90.00</td>'>,\n",
    " <Selector xpath=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' precio ')]\" data='<td class=\"precio\">$105.00</td>'>,\n",
    " <Selector xpath=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' precio ')]\" data='<td class=\"precio\">$145.00</td>'>,\n",
    " <Selector xpath=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' precio ')]\" data='<td class=\"precio\">$100.00</td>'>,\n",
    " <Selector xpath=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' precio ')]\" data='<td class=\"precio\">$160.00</td>'>,\n",
    " <Selector xpath=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' precio ')]\" data='<td class=\"precio\">$220.00</td>'>,\n",
    " <Selector xpath=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' precio ')]\" data='<td class=\"precio\">$95.00</td>'>,\n",
    " <Selector xpath=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' precio ')]\" data='<td class=\"precio\">$65.00</td>'>,\n",
    " <Selector xpath=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' precio ')]\" data='<td class=\"precio\">$85.00</td>'>,\n",
    " <Selector xpath=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' precio ')]\" data='<td class=\"precio\">$115.00</td>'>]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para desplegar el texto de cada nodo encontrado se aplica el siguiente código:\n",
    "\n",
    "``` python\n",
    "for item in response.css('.precio'):\n",
    "    print(item.extract())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de una búsqueda.\n",
    "\n",
    "Añadir lo siguiente al archivo [prueba/items.py](prueba/items.py).\n",
    "\n",
    "``` python\n",
    "from scrapy.item import Item, Field\n",
    "\n",
    "class PruebaItem(Item):\n",
    "    # Primary fields\n",
    "    platillo = Field()\n",
    "    precio = Field()\n",
    "```\n",
    "\n",
    "Añadir lo siguiente al archivo [prueba/spiders/coder.py](prueba/spiders/coder.py).\n",
    "\n",
    "``` python\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "from prueba.items import PruebaItem\n",
    "\n",
    "class CoderSpider(scrapy.Spider):\n",
    "    name = 'coder'\n",
    "    allowed_domains = ['coder.mx']\n",
    "    start_urls = ['http://coder.mx/menu.html']\n",
    "\n",
    "    def parse(self, response):\n",
    "        item = PruebaItem()\n",
    "        item['platillo'] = response.xpath('//dt').extract()\n",
    "        item['precio'] = response.css('.precio').extract()\n",
    "        return item\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/oi/py121_Datos_con_Python/prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy crawl coder -o items.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy fetch --spider=coder http://coder.mx/menu.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<p style=\"text-align: center\"><a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Licencia Creative Commons\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/80x15.png\" /></a><br />Esta obra está bajo una <a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\">Licencia Creative Commons Atribución 4.0 Internacional</a>.</p>\n",
    "<p style=\"text-align: center\">&copy; José Luis Chiquete Valdivieso. 2018.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
